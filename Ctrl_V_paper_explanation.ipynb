{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7XgJHqEjub3Te9B1Wf2nb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaseelkt007/Generative_AI_basics/blob/main/Ctrl_V_paper_explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ctrl-V paper Explanation:\n",
        "**Ctrl-V: Higher Fidelity Video Generation with Bounding-Box Controlled Object Motion (Dec 2024)**\n",
        "\n",
        "https://arxiv.org/abs/2406.05630\n",
        "\n",
        "Ctrl-V uses two-part approach :\n",
        "1. It first generates trajectories for object bounding boxes over time (BBox Generator using a diffusion model).\n",
        "2. Then, it generates the actual video (Box2Video) conditioned on these bounding box trajectories using a specialized video diffusion model.\n",
        "    *   In the **Box2Video** step, Ctrl-V directly adapts the ControlNet framework for video generation:\n",
        "        *   The ControlNet takes “bounding box frames” as control signals and injects them into the video diffusion backbone (Stable Video Diffusion, SVD).\n",
        "        *   The SVD’s original weights are frozen, and only the ControlNet-adapter module is trained—mirroring the original image ControlNet design but extended for video synthesis and spatiotemporal control.\n",
        "\n",
        "\n",
        "\n",
        "# BBox Generator\n",
        "\n",
        "## 1. Purpose\n",
        "\n",
        "The BBox Generator predicts bounding boxes for objects across all video frames, using an SVD (likely Stable Video Diffusion or space-time video diffusion) backbone.\n",
        "\n",
        "## 2. Inputs to the Model\n",
        "\n",
        "There are four main inputs:\n",
        "\n",
        "### $\\hat{b}^t$\n",
        "- The encoded \"video\" of bounding boxes, but with t levels of noise added\n",
        "- In training, this is what the model tries to denoise\n",
        "\n",
        "### $b^{(0)}$\n",
        "- The encoded initial bounding box/frame(s)\n",
        "- Represents the object position(s) at the start of the video\n",
        "\n",
        "### $b^{(N-1)}$\n",
        "- The encoded final bounding box/frame\n",
        "- The object position(s) at the end of the video\n",
        "\n",
        "### $z^{(0)}$\n",
        "- The encoded initial video frame itself (not the bounding box, but the frame's latent or pixel-space encoding)\n",
        "\n",
        "## 3. Training Objective\n",
        "\n",
        "### Goal\n",
        "The model learns to predict the noise that was added to $\\hat{b}^t$ (the noisy boxes) using the UNet in an EDM (Elucidated Diffusion Model) noise schedule.\n",
        "\n",
        "### Process\n",
        "It recovers the original bounding boxes $b$ from the noisy version $\\hat{b}^t$ by:\n",
        "- Predicting the noise component (using UNet outputs)\n",
        "- \"Eliminating\" this noise using certain scaling functions (common in diffusion models)\n",
        "\n",
        "*Note: Model diagram abstracts away this denoising detail for simplicity.*\n",
        "\n",
        "## 4. Input Vector Formation\n",
        "\n",
        "All four inputs are transformed and concatenated into the expected format for the UNet adapter inside the SVD backbone.\n",
        "\n",
        "### Key Step – Constructing $z_{pad}^{(0)}$\n",
        "\n",
        "- $z^{(0)}$ has a shape $1 \\times C' \\times H' \\times W'$ (one frame, with feature channels and spatial dimensions)\n",
        "- It is replicated to get shape $N \\times C' \\times H' \\times W'$ (for N frames)\n",
        "\n",
        "**Crucially:**\n",
        "The very first (0-th) and last (N-1-th) elements in the first (frame) dimension are replaced:\n",
        "- The first becomes $b^{(0)}$ (initial bounding box)\n",
        "- The last becomes $b^{(N-1)}$ (final bounding box)\n",
        "\n",
        "This process creates a padded tensor:\n",
        "\n",
        "$$z_{pad}^{(0)} = \\text{concat}(b^{(0)}, z^{(0)}, ..., z^{(0)}, b^{(N-1)})$$\n",
        "\n",
        "**Shape:** $N \\times C' \\times H' \\times W'$\n",
        "\n",
        "## 5. Final Model Input\n",
        "\n",
        "The noisy bounding box encoding $\\hat{b}^t$ is concatenated along with $z_{pad}^{(0)}$.\n",
        "\n",
        "This combined tensor is fed to the UNet adapter.\n",
        "\n",
        "## 6. Additional Conditioning\n",
        "\n",
        "The model also uses extra info for conditioning at each UNet layer/block:\n",
        "\n",
        "### $c^{(0)}$ - CLIP-encoded embedding of the initial frame\n",
        "- CLIP features (semantic info) of the starting frame\n",
        "\n",
        "### $t$ - Noise-level embedding\n",
        "- Indicates the noise intensity (typical in diffusion models)\n",
        "\n",
        "These embeddings are integrated into every sub-block of the UNet using a self-attention mechanism (so every part of the network is aware of them throughout the process).\n",
        "\n",
        "## Summary Table\n",
        "\n",
        "| Component | Role |\n",
        "|-----------|------|\n",
        "| $\\hat{b}^t$ | Noisy video of bounding box encodings (to be denoised by model) |\n",
        "| $b^{(0)}$ | Encoded initial bounding box/frame |\n",
        "| $b^{(N-1)}$ | Encoded final bounding box/frame |\n",
        "| $z^{(0)}$ | Encoded initial video frame |\n",
        "| $z_{pad}^{(0)}$ | N-frame tensor, start and end replaced by $b^{(0)}$, $b^{(N-1)}$, rest are $z^{(0)}$ |\n",
        "| UNet Adapter Input | Concatenation of $\\hat{b}^t$ and $z_{pad}^{(0)}$ |\n",
        "| Conditioning Inputs | CLIP embedding of initial frame ($c^{(0)}$), noise level embedding ($t$) |\n",
        "| Integration | Conditionings are fed into every UNet sub-block via self-attention |\n",
        "\n",
        "## In Short\n",
        "\n",
        "The BBox Generator uses noisy and clean cues from bounding boxes and video frames.\n",
        "\n",
        "It \"pads\" a stack of latent encodings with the initial and final bounding box states.\n",
        "\n",
        "It concatenates this with the noisy box encoding for UNet-based denoising.\n",
        "\n",
        "Semantic and noise-level information are injected via self-attention throughout the network."
      ],
      "metadata": {
        "id": "2sXUsMTy9TjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Representing Bounding Box in Pixel space\n",
        "\n",
        "## 1.Ctrl-V Design Choice: Pixel-Space Rendering\n",
        "\n",
        "**Ctrl-V** renders **bounding boxes as actual images/frames in pixel space**, not just as numbers (coordinates).\n",
        "\n",
        "## 2. Why is this important?\n",
        "\n",
        "**How** bounding box information is encoded and injected as a control signal for the video generator **matters a lot** for generation performance and flexibility.\n",
        "\n",
        "### Prior approach example (Boximator):\n",
        "- Bounding boxes are converted into a **vector format** (by using the Fourier transform on raw box coordinates, plus ID and metadata)\n",
        "- This is a compact, vectorized (not image) representation used as input\n",
        "\n",
        "## 3. Ctrl-V's Contrasting Approach\n",
        "\n",
        "Instead of vectorizing, **Ctrl-V renders bounding boxes into images (frames)**, intentionally preserving spatial and meta information visually.\n",
        "\n",
        "### What does this mean?\n",
        "- The bounding box for each object is literally drawn into a \"canvas\" (image) as it would appear in the original frame\n",
        "- This is done for every frame in the sequence\n",
        "\n",
        "## 4. Encoding Extra Information Visually\n",
        "\n",
        "The following information is encoded visually in the rendered bounding boxes:\n",
        "\n",
        "### Track ID\n",
        "- Uniquely identifies each object throughout the video\n",
        "\n",
        "### Object Type\n",
        "- Tells what kind of object it is (e.g., car, pedestrian)\n",
        "\n",
        "### Orientation\n",
        "- Indicates which direction the object is facing\n",
        "\n",
        "### How are these encoded?\n",
        "\n",
        "| Visual Element | Encodes |\n",
        "|----------------|---------|\n",
        "| **Border color** | Object's ID |\n",
        "| **Fill color** | Type of object |\n",
        "| **Markings** | Orientation (could be an arrow, angled line, etc.) |\n",
        "\n",
        "## 5. Advantages of This Approach\n",
        "\n",
        "### Minimal Loss of Meta-information\n",
        "- By rendering everything visually in the frame, you don't \"throw away\" any of the information that coordinatization or vectorization might lose (e.g., relative positions, shapes, types)\n",
        "\n",
        "### Pixel-Level Guidance using ControlNet\n",
        "- Since the box is rendered into a pixel-level frame, it is possible to use **ControlNet** (a diffusion model guidance method) to impose *precise* control over what parts of the image or video the diffusion process should focus on"
      ],
      "metadata": {
        "id": "zIO3CLEWMtt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Box2Video\n",
        "\n",
        "## Purpose of Box2Video\n",
        "\n",
        "**Goal:** To generate *high-fidelity videos* that are **controlled** by bounding box frames.\n",
        "- Bounding boxes may come from the BBox Generator\n",
        "\n",
        "## Architecture at a Glance\n",
        "\n",
        "### Components:\n",
        "- **SVD Backbone:** Handles video diffusion/generation (SVD = Stable Video Diffusion or similar)\n",
        "- **Adapted ControlNet Module:** Processes the bounding box control signal and injects it into the video diffusion process\n",
        "\n",
        "## ControlNet\n",
        "\n",
        "- A network for **controlling image (now video) generation** using explicit signals (e.g., bounding boxes in image/pixel space)\n",
        "- **Modification:** Ctrl-V adapts ControlNet so it integrates with a video diffusion model instead of just images\n",
        "\n",
        "## Training Efficiency\n",
        "\n",
        "### Single-stage training\n",
        "- Ctrl-V's Box2Video is trained end-to-end (single stage), *without* extra losses/criteria or staged pretraining\n",
        "\n",
        "### Prior work comparison:\n",
        "- Previous methods (Boximator, TrackDiffusion) **required extra multi-stage training** and extra losses\n",
        "- Box2Video is architected for simpler, more efficient training\n",
        "\n",
        "## Inputs and Pre-processing\n",
        "\n",
        "### Two main video-related inputs to SVD:\n",
        "- $z^{(0)}$: Encoded initial video frame\n",
        "- $z^t$: Encoded entire video (with noise level t added)\n",
        "\n",
        "### Preparation:\n",
        "- $z^{(0)}$ is **padded (repeated along the time/frame dimension)** so shapes match for concatenation\n",
        "- These are concatenated to form input to the SVD's UNet adapter (video diffusion entry)\n",
        "\n",
        "## ControlNet Processing Path\n",
        "\n",
        "### Parallel Inputs:\n",
        "- The **same input** (concatenated [$z^{(0)}$, $z^t$]) is also sent into ControlNet via its own UNet adapter layers\n",
        "- **Bounding box frames $b$:** (Rendered in pixel space as previously discussed) are also encoded and sent into ControlNet through special adapter (\"ControlNet adapter layers\")\n",
        "\n",
        "### Merging:\n",
        "- The two ControlNet input streams (**video** and **bounding box frames**) are *added together*\n",
        "- This combined information is processed by ControlNet, allowing the network to use **both structure (boxes) and context (video init/content)** to guide generation\n",
        "\n",
        "## Output Flow & Control Signal Injection\n",
        "\n",
        "- The processed signal from ControlNet goes through a **zero-convolution** (which usually means a learnable 1×1 convolution initialized to zero, so it starts with no contribution and gradually learns to influence)\n",
        "\n",
        "### Residual Path:\n",
        "- This \"control\" signal is injected (residually) into the **SVD UNet decoder layers**, influencing each stage of the video generation process according to the control input\n",
        "\n",
        "## Training Details\n",
        "\n",
        "| Component | Training Status | Description |\n",
        "|-----------|----------------|-------------|\n",
        "| **SVD weights ($\\theta$)** | **Frozen** | The main video diffusion model doesn't learn further |\n",
        "| **ControlNet weights ($\\xi$)** | **Trained** | Only these weights are updated during training |\n",
        "\n",
        "This allows the model to efficiently *adapt* or *guide* the base diffusion model using bounding boxes, without retraining the whole video generator from scratch."
      ],
      "metadata": {
        "id": "Q0F3BpX4RtM8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_whyGYKQ9S_S"
      },
      "outputs": [],
      "source": []
    }
  ]
}